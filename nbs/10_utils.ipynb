{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "> Useful function and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "import requests\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from mimetypes import types_map\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Union\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core import magic\n",
    "from IPython.display import Javascript\n",
    "from ipywidgets import Output, VBox, Button, HTML\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "PathStr = Union[Path, str]\n",
    "PathURL = Union[Path, str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def is_jupyter():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False  # Probably standard Python interpreter\n",
    "    \n",
    "IS_JUPYTER = is_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "logger = logging.getLogger(\"unpackai.utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# To be able to run the tests in the Notebook\n",
    "from pathlib import Path\n",
    "import ipytest\n",
    "import sys\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "root_dir = Path(\"..\").resolve()\n",
    "sys.path.append(str(root_dir / \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "# For Test Cases (might have duplicate import because it will be in a dedicated file)\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from shutil import copy, rmtree\n",
    "from typing import List\n",
    "\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from PIL import Image\n",
    "from test_common.utils_4_tests import DATA_DIR, IMG_DIR, check_no_log, check_only_warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up error image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we have error image that will interrupt model trainging. We can use ```clean_error_img``` to clean the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_img(\n",
    "    img: Path,\n",
    "    formats: List[str] = [\".jpg\", \".jpeg\", \".png\", \".bmp\"],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Check on a single image,\n",
    "    If it's quality is troublesome\n",
    "        we unlink/ditch the image\n",
    "    \"\"\"\n",
    "    img = Path(img)\n",
    "    # check if this path is an image\n",
    "    if img.suffix.lower().split(\"?\")[0] not in formats:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # try to open that image and then (to check truncated image)\n",
    "        # We need to include in a with block to close the image before deleting\n",
    "        with Image.open(img) as im:\n",
    "            im.load()\n",
    "    except Exception:\n",
    "        if img.exists():\n",
    "            img.unlink()\n",
    "            logging.warning(f\"Removed erroneous img: {img}\")\n",
    "            return\n",
    "\n",
    "\n",
    "def clean_error_img(\n",
    "    path: Path,\n",
    "    progress: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    - path: an image directory\n",
    "    - progress: do we print out progress bar or not\n",
    "        default True\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    # check directory existence\n",
    "    if path.exists() == False:\n",
    "        raise FileExistsError(\n",
    "            f\"\"\"path does not exists on:{path}, \n",
    "    make sure there is a directory \"{path.name}\".\n",
    "    under directory \"{path.parent}\"\n",
    "    \"\"\")\n",
    "\n",
    "    # create iterator, probably with progress bar\n",
    "    iterator = tqdm(list(path.iterdir()), leave=False)\\\n",
    "        if progress else path.iterdir()\n",
    "\n",
    "    for obj in iterator:\n",
    "        if obj.is_dir():\n",
    "            # use recursion to clean the sub folder\n",
    "            clean_error_img(obj, progress=progress)\n",
    "        else:\n",
    "            # cheking on a single image\n",
    "            check_img(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "\n",
    "images_rob = list((IMG_DIR / \"robustness\").glob(\"*.*\"))\n",
    "IMG_RGB = Image.new(\"RGB\", (60, 30), color=(73, 109, 137))\n",
    "IMG_RGBA = Image.new(\"RGBA\", (60, 30), color=(73, 109, 137, 100))\n",
    "\n",
    "@pytest.mark.parametrize(\"img\", images_rob, ids=[i.name for i in images_rob])\n",
    "def test_check_img_error(img: Path, tmpdir, caplog):\n",
    "    \"\"\"Test check_img with incorrect images\"\"\"\n",
    "    img_copy = Path(tmpdir) / img.name\n",
    "    copy(img, img_copy)\n",
    "    check_img(img_copy)\n",
    "    check_only_warning(caplog, img.name)\n",
    "    assert not img_copy.is_file(), f\"File {img_copy} not be deleted\"\n",
    "\n",
    "\n",
    "def test_check_img_empty_wrong_suffix(tmpdir, caplog):\n",
    "    \"\"\"Test check_img with wrong image that does not have a correct extension\"\"\"\n",
    "    img_path = Path(tmpdir) / \"empty.txt\"\n",
    "    img_path.write_bytes(b\"\")\n",
    "    check_img(img_path)\n",
    "    check_no_log(caplog)\n",
    "    assert img_path.is_file(), f\"Image {img_path} not found\"\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"img\", [IMG_RGB, IMG_RGBA])\n",
    "@pytest.mark.parametrize(\"ext\", [\"png\", \"bmp\", \"jpg\", \"jpeg\"])\n",
    "def test_check_img_correct(img: Image, ext: str, tmpdir, caplog):\n",
    "    \"\"\"Correct that correct image is not removed\"\"\"\n",
    "    alpha_suffix = \"_alpha\" if len(img.getcolors()[0][1]) == 4 else \"\"\n",
    "    if alpha_suffix and ext.startswith(\"jp\"):\n",
    "        pytest.skip(\"JPG does not support RGBA\")\n",
    "\n",
    "    img_path = Path(tmpdir) / f\"correct_image_blue{alpha_suffix}.{ext}\"\n",
    "    img.save(img_path)\n",
    "    check_img(img_path)\n",
    "    check_no_log(caplog)\n",
    "    assert img_path.is_file(), f\"Image {img_path} not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "\n",
    "def test_clean_error_img(tmpdir, monkeypatch) -> None:\n",
    "    \"\"\"Test clean_error_img\"\"\"\n",
    "    for img in images_rob:\n",
    "        copy(img, Path(tmpdir) / img.name)\n",
    "\n",
    "    monkeypatch.chdir(tmpdir)\n",
    "    root = Path(\".\")\n",
    "\n",
    "    sub1 = root / \"sub1\"\n",
    "    sub2 = root / \"sub2\"\n",
    "    sub1.mkdir()\n",
    "    sub2.mkdir()\n",
    "    sub3 = sub2 / \"sub3\"\n",
    "    sub3.mkdir()\n",
    "\n",
    "\n",
    "    (sub1 / \"file11.BMP\").write_text(\"fake image\")\n",
    "    (sub1/\"ðŸ˜±file12 haha.jpg\").write_text(\"fake image\")\n",
    "    (sub1 / \"file13 haha.txt\").write_text(\"not image\")\n",
    "    IMG_RGB.save(sub1 / \"img14_good.jpg\")\n",
    "    IMG_RGBA.save(sub1 / \"img15_good.png\")\n",
    "\n",
    "    (sub2 / \"file21.jpg\").write_text(\"fake image\")\n",
    "    (sub2 / \"file22.jpeg\").write_text(\"fake image\")\n",
    "\n",
    "    (sub3 / \"file31.jpeg\").write_text(\"fake image\")\n",
    "    IMG_RGB.save(sub3 / \"img32_good.jpeg\")\n",
    "    IMG_RGBA.save(sub3 / \"img33_good.bmp\")\n",
    "\n",
    "\n",
    "    good: List[Path] = list()\n",
    "    bad: List[Path] = list()\n",
    "\n",
    "    print(\"Existing files:\")\n",
    "    for file in root.rglob(\"*.*\"):\n",
    "        print(f\" * {file}\")\n",
    "        if file.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
    "            (good if \"good\" in file.name else bad).append(file)\n",
    "\n",
    "    print(\" => CLEANING\")\n",
    "    clean_error_img(root, progress=False)\n",
    "\n",
    "    good_removed = [f for f in good if not f.is_file()]\n",
    "    assert not good_removed, f\"Good pictures deleted: {good_removed}\"\n",
    "\n",
    "    bad_still_here = [f for f in bad if f.is_file()]\n",
    "    assert not bad_still_here, f\"Bad pictures not deleted: {bad_still_here}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic function ```hush```\n",
    "> Run things quietly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a magical cell function, so remember to use the ```%%```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 versions of hush, the 1st on is the backup one, that using ipywidget as event trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# @register_cell_magic\n",
    "def hush(line, cell):\n",
    "    \"\"\"\n",
    "    A magic cell function to collapse the print out\n",
    "    %%hush\n",
    "    how_loud = 100\n",
    "    how_verbose = \"very\"\n",
    "    do_some_python_thing(how_loud, how_verbose)\n",
    "    \"\"\"\n",
    "    # the current output widget\n",
    "    out = Output()\n",
    "    output_box = VBox([out])\n",
    "    # default setting is to hide the print out\n",
    "    output_box.layout.display = \"none\"\n",
    "    # the toggling button\n",
    "    show_btn = Button(description=\"Show output\")\n",
    "\n",
    "    def toggle_output(o):\n",
    "        # show output, change the button to hide\n",
    "        if output_box.layout.display == \"none\":\n",
    "            output_box.layout.display = \"block\"\n",
    "            show_btn.description = \"Hide output\"\n",
    "        # hide output, change the button to show\n",
    "        else:\n",
    "            output_box.layout.display = \"none\"\n",
    "            show_btn.description = \"Show output\"\n",
    "\n",
    "    # assign toggle to event\n",
    "    show_btn.on_click(toggle_output)\n",
    "\n",
    "    # A control panel containing a button\n",
    "    # and the output box\n",
    "    total_control = VBox([show_btn, output_box])\n",
    "    display(total_control)\n",
    "\n",
    "    with out:\n",
    "        ishell = get_ipython()\n",
    "        # excute the code in cell\n",
    "        result = ishell.run_cell(\n",
    "            cell, silent=False)\n",
    "\n",
    "    # we still want the error to be proclaimed loudly\n",
    "    if result.error_in_exec:\n",
    "        logging.error(f\"'{result.error_in_exec}' happened, breaking silence now\")\n",
    "        result.raise_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this version of ```toggle action``` will be stuck by the ongoing interactive\n",
    "\n",
    "If the process is working on some thing, you can't toggle until the end of the execution.\n",
    "\n",
    "The further improvement will be move the toggle entirely to JavaScript, hence the second version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hush event in JS\n",
    "> As not going through any amount of python backend after run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hush(line, cell):\n",
    "    \"\"\"\n",
    "    A magic cell function to collapse the print out\n",
    "    %%hush\n",
    "    how_loud = 100\n",
    "    how_verbose = \"very\"\n",
    "    do_some_python_thing(how_loud, how_verbose)\n",
    "    \"\"\"\n",
    "    # the current output widget\n",
    "    out = Output()\n",
    "    output_box = VBox([out])\n",
    "    \n",
    "    # create uuid for DOM identifying\n",
    "    uuid = str(uuid4())\n",
    "\n",
    "    # default setting is to hide the print out\n",
    "    output_box.layout.display = \"none\"\n",
    "    # the toggling button\n",
    "    show_btn = Button(description=\"toggle output\")\n",
    "    show_btn.add_class(f\"hush_toggle_{uuid}\")\n",
    "    output_box.add_class(f\"hush_output_{uuid}\")\n",
    "    \n",
    "    \n",
    "    # A control panel containing a button\n",
    "    # and the output box\n",
    "    total_control = VBox([show_btn, output_box])\n",
    "    display(total_control)\n",
    "    \n",
    "    # assign JS listener\n",
    "    display(Javascript(f\"\"\"\n",
    "    console.info(\"loading toggle event: {uuid}\")\n",
    "    const toggle_hush = (e) =>\\u007b\n",
    "        var op = document.querySelector(\".hush_output_{uuid}\");\n",
    "        if(op.style.display==\"none\")\\u007b\n",
    "             op.style.display=\"block\"\n",
    "\n",
    "        \\u007d else \\u007b\n",
    "            op.style.display=\"none\"\n",
    "        \\u007d\n",
    "\n",
    "    \\u007d\n",
    "    document.querySelector('.hush_toggle_{uuid}').onclick=toggle_hush\n",
    "    \"\"\"))\n",
    "\n",
    "    with out:\n",
    "        ishell = get_ipython()\n",
    "        # excute the code in cell\n",
    "        result = ishell.run_cell(\n",
    "            cell, silent=False)\n",
    "\n",
    "    # we still want the error to be proclaimed loudly\n",
    "    if result.error_in_exec:\n",
    "        logging.error(f\"'{result.error_in_exec}' happened, breaking silence now\")\n",
    "        result.raise_error()\n",
    "        \n",
    "if IS_JUPYTER:\n",
    "    magic.register_cell_magic(hush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try hushing various kinds of info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html display\n",
    "* print\n",
    "* logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7250947c254eb394b84a5caad1bc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='toggle output', style=ButtonStyle(), _dom_classes=('hush_toggle_00d9724d-55â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    console.info(\"loading toggle event: 00d9724d-55e4-4ab8-b426-5e92f71f593c\")\n",
       "    const toggle_hush = (e) =>{\n",
       "        var op = document.querySelector(\".hush_output_00d9724d-55e4-4ab8-b426-5e92f71f593c\");\n",
       "        if(op.style.display==\"none\"){\n",
       "             op.style.display=\"block\"\n",
       "\n",
       "        } else {\n",
       "            op.style.display=\"none\"\n",
       "        }\n",
       "\n",
       "    }\n",
       "    document.querySelector('.hush_toggle_00d9724d-55e4-4ab8-b426-5e92f71f593c').onclick=toggle_hush\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%hush\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.getLogger().setLevel('DEBUG')\n",
    "a = 1\n",
    "for i in range(1000):\n",
    "    print(i, end=\"\\t\")\n",
    "    \n",
    "logging.info(\"a lots of text, VERBOSITY!!\"*100)\n",
    "\n",
    "# a big dataframe\n",
    "display(pd.DataFrame({\"a\":[1,2]*50}))\n",
    "\n",
    "# a big output\n",
    "pd.DataFrame({\"b\":range(100)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capable of open/close verbosity during the main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b605b939cae43bb850addb23125e4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='toggle output', style=ButtonStyle(), _dom_classes=('hush_toggle_a5461483-beâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    console.info(\"loading toggle event: a5461483-be0b-4a73-8299-458f05a7c0d5\")\n",
       "    const toggle_hush = (e) =>{\n",
       "        var op = document.querySelector(\".hush_output_a5461483-be0b-4a73-8299-458f05a7c0d5\");\n",
       "        if(op.style.display==\"none\"){\n",
       "             op.style.display=\"block\"\n",
       "\n",
       "        } else {\n",
       "            op.style.display=\"none\"\n",
       "        }\n",
       "\n",
       "    }\n",
       "    document.querySelector('.hush_toggle_a5461483-be0b-4a73-8299-458f05a7c0d5').onclick=toggle_hush\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%hush\n",
    "from time import sleep\n",
    "for i in tqdm(range(10)):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we still want the error to be loud\n",
    "> As such information should interrupt the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e89cb009165465ab33b2bbc77067d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='toggle output', style=ButtonStyle(), _dom_classes=('hush_toggle_33d84ffd-6fâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    console.info(\"loading toggle event: 33d84ffd-6fef-48a7-8ddc-26ef5ec8513b\")\n",
       "    const toggle_hush = (e) =>{\n",
       "        var op = document.querySelector(\".hush_output_33d84ffd-6fef-48a7-8ddc-26ef5ec8513b\");\n",
       "        if(op.style.display==\"none\"){\n",
       "             op.style.display=\"block\"\n",
       "\n",
       "        } else {\n",
       "            op.style.display=\"none\"\n",
       "        }\n",
       "\n",
       "    }\n",
       "    document.querySelector('.hush_toggle_33d84ffd-6fef-48a7-8ddc-26ef5ec8513b').onclick=toggle_hush\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%hush\n",
    "def _test_hush():\n",
    "    for i in range(20):\n",
    "        print(f\"some logging:\\t{i}!\")\n",
    "    raise ValueError(\"but some error, because life!\")\n",
    "\n",
    "# Testing hush when exception is raised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching static files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def static_root() -> Path:\n",
    "    \"\"\"\n",
    "    Return static path\n",
    "    \"\"\"\n",
    "    import unpackai\n",
    "\n",
    "    unpackai_path = Path(unpackai.__path__[0])\n",
    "    if not unpackai_path.exists():\n",
    "        egg_path = Path(f\"{unpackai_path}.egg-link\")\n",
    "        unpackai_path = Path(egg_path.read_text())\n",
    "\n",
    "    return unpackai_path / \"static\"\n",
    "\n",
    "\n",
    "STATIC = static_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the validation of such static path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "def test_find_static():\n",
    "    error_report_html = STATIC/\"html\"/\"bug\"/\"error_report.html\"\n",
    "    assert (error_report_html).is_file(), f\"'{STATIC}' is not a valid static path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing files in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def _iter_files(root: Path, exclude_dir: List[str], hide_info: bool):\n",
    "    \"\"\"Return all info of files found in a root directory\"\"\"\n",
    "    for f in root.rglob(\"*\"):\n",
    "        if any(d in f.parts for d in exclude_dir):\n",
    "            continue\n",
    "\n",
    "        info = {\n",
    "            \"Name\": f.name,\n",
    "            \"Parent\": f.parent.name,\n",
    "            \"Path\": f.as_posix(),\n",
    "            \"Level\": len(f.relative_to(root).parent.parts),\n",
    "            \"Last_Modif\": datetime.fromtimestamp(f.stat().st_mtime),\n",
    "        }\n",
    "        if f.is_file():\n",
    "            info.update(\n",
    "                {\n",
    "                    \"FileDir\": \"File\",\n",
    "                    \"Extension\": f.suffix or f.name,\n",
    "                    \"Type\": types_map.get(f.suffix.lower()),\n",
    "                }\n",
    "            )\n",
    "            if not hide_info:\n",
    "                size = f.stat().st_size\n",
    "                info.update(\n",
    "                    {\n",
    "                        \"Size\": size,\n",
    "                        \"Friendly_Size\": friendly_size(size),\n",
    "                    }\n",
    "                )\n",
    "        elif f.is_dir():\n",
    "            info.update({\"FileDir\": \"Dir\"})\n",
    "\n",
    "        yield info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def friendly_size(size: float) -> str:\n",
    "    \"\"\"Convert a size in bytes (as float) to a size with unit (as a string)\"\"\"\n",
    "    unit = \"B\"\n",
    "    # Reminder: 1 KB = 1024 B, and 1 MB = 1024 KB, ...\n",
    "    for letter in \"KMG\":\n",
    "        if size >= 1024:\n",
    "            size /= 1024\n",
    "            unit = f\"{letter}B\"\n",
    "\n",
    "    # We want to keep 2 digits after floating point\n",
    "    # because it is a good balance between precision and concision\n",
    "    return f\"{size:0.2f} {unit}\"\n",
    "\n",
    "\n",
    "def ls(root: PathStr, exclude: List[str] = None, hide_info=False) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame with list of files & directories in a given path.\n",
    "\n",
    "    Args:\n",
    "        root: root path to look for files and directories recursively\n",
    "        exclude: optional list of names to exclude in the search (e.g. `[\".git\"]`)\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    if not root.is_dir():\n",
    "        raise FileNotFoundError(f\"Path {root} does not exist or is not a directory\")\n",
    "    df = pd.DataFrame(\n",
    "        _iter_files(root, exclude_dir=exclude or [], hide_info=hide_info)\n",
    "    )\n",
    "    return df.sort_values(by=[\"Path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "@pytest.mark.parametrize(\n",
    "    \"size,exp\",\n",
    "    [\n",
    "        (0, \"0.00 B\"),\n",
    "        (1, \"1.00 B\"),\n",
    "        (1024, \"1.00 KB\"),\n",
    "        (1024 ** 2, \"1.00 MB\"),\n",
    "        (1024 ** 3 * 3.14, \"3.14 GB\"),\n",
    "    ],\n",
    ")\n",
    "def test_friendly_size(size, exp):\n",
    "    \"\"\"Test computation of friendly size (human readable)\"\"\"\n",
    "    assert friendly_size(size) == exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "@pytest.fixture\n",
    "def populated_tmp_dir(tmpdir):\n",
    "    \"\"\"Create files to test `ls` function\"\"\"\n",
    "    tmpdir = Path(tmpdir)\n",
    "    for dir_ in [\"dir1/subdir1\", \"dir1/subdir2\", \"dir2\"]:\n",
    "        (tmpdir / dir_).mkdir(parents=True)\n",
    "    for file in [\"at_root.txt\", \"dir1/subdir1/at_subdir.txt\", \"dir2/at_dir.txt\"]:\n",
    "        (tmpdir / file).write_text(\"unpackai\")\n",
    "    (tmpdir / \"dir1\" / \"subdir2\" / \"some_pickle.pkl\").write_bytes(b\"3141590000\")\n",
    "\n",
    "    return tmpdir\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "exp_columns = [\n",
    "    \"Name\",\n",
    "    \"Parent\",\n",
    "    \"Path\",\n",
    "    \"Level\",\n",
    "    \"Last_Modif\",\n",
    "    \"FileDir\",\n",
    "    \"Extension\",\n",
    "    \"Type\",\n",
    "    \"Size\",\n",
    "    \"Friendly_Size\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_ls(populated_tmp_dir):\n",
    "    \"\"\"Test DataFrame generated by `ls` function\"\"\"\n",
    "    df = ls(populated_tmp_dir)\n",
    "    now = datetime.now()\n",
    "    assert list(df.columns) == exp_columns, \"Incorrect columns in DF\"\n",
    "\n",
    "    # We want to copy the list of expecgted columns to keep it intact\n",
    "    columns = exp_columns[:]\n",
    "\n",
    "    assert all(now - timedelta(minutes=5) < date < now for date in df[\"Last_Modif\"])\n",
    "    df.drop(\"Last_Modif\", axis=1, inplace=True)\n",
    "    columns.remove(\"Last_Modif\")\n",
    "\n",
    "    # We check the file \"at_root.txt\" and do some cleanup\n",
    "    at_root = df[df[\"Name\"] == \"at_root.txt\"].iloc[0]\n",
    "    assert Path(at_root[\"Path\"]) == populated_tmp_dir.absolute() / \"at_root.txt\"\n",
    "    assert at_root[\"Friendly_Size\"] == friendly_size(at_root[\"Size\"])\n",
    "    df.drop([\"Path\", \"Friendly_Size\"], axis=1, inplace=True)\n",
    "    columns.remove(\"Path\")\n",
    "    columns.remove(\"Friendly_Size\")\n",
    "\n",
    "    print(\"===TRUNCATED DF FOR LIST OF FILES/DIR===\")\n",
    "    print(df)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    exp_root_dir = populated_tmp_dir.name\n",
    "    df_exp = pd.DataFrame(\n",
    "        [\n",
    "            (\"at_root.txt\", exp_root_dir, 0, \"File\", \".txt\", \"text/plain\", 8.0),\n",
    "            (\"dir1\", exp_root_dir, 0, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"subdir1\", \"dir1\", 1, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"at_subdir.txt\", \"subdir1\", 2, \"File\", \".txt\", \"text/plain\", 8.0),\n",
    "            (\"subdir2\", \"dir1\", 1, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"some_pickle.pkl\", \"subdir2\", 2, \"File\", \".pkl\", None, 10.0),\n",
    "            (\"dir2\", exp_root_dir, 0, \"Dir\", np.NaN, np.NaN, np.NaN),\n",
    "            (\"at_dir.txt\", \"dir2\", 1, \"File\", \".txt\", \"text/plain\", 8.0),\n",
    "        ],\n",
    "        columns=columns,\n",
    "    )\n",
    "    print(\"===EXPECTED DF FOR LIST OF FILES/DIR===\")\n",
    "    print(df_exp)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    compare_df = df.reset_index(drop=True).compare(df_exp.reset_index(drop=True))\n",
    "    assert compare_df.empty, f\"Differences found when checking DF:\\n{compare_df}\"\n",
    "\n",
    "\n",
    "exp_files = [\n",
    "    \"at_root.txt\",\n",
    "    \"dir1\",\n",
    "    \"subdir1\",\n",
    "    \"at_subdir.txt\",\n",
    "    \"subdir2\",\n",
    "    \"some_pickle.pkl\",\n",
    "    \"dir2\",\n",
    "    \"at_dir.txt\",\n",
    "]\n",
    "exp_dir1 = [\"at_root.txt\", \"dir2\", \"at_dir.txt\"]\n",
    "exp_dir2 = [\n",
    "    \"at_root.txt\",\n",
    "    \"dir1\",\n",
    "    \"subdir1\",\n",
    "    \"subdir2\",\n",
    "    \"at_subdir.txt\",\n",
    "    \"some_pickle.pkl\",\n",
    "]\n",
    "exp_subdir = [\n",
    "    \"at_root.txt\",\n",
    "    \"dir1\",\n",
    "    \"dir2\",\n",
    "    \"subdir2\",\n",
    "    \"some_pickle.pkl\",\n",
    "    \"at_dir.txt\",\n",
    "]\n",
    "exp_dir2_subdir = [\"at_root.txt\", \"dir1\", \"subdir2\", \"some_pickle.pkl\"]\n",
    "\n",
    "\n",
    "def test_ls_path_as_str(populated_tmp_dir):\n",
    "    \"\"\"Test `ls` function called with a string for path\"\"\"\n",
    "    df = ls(str(populated_tmp_dir))\n",
    "    assert sorted(df[\"Name\"]) == sorted(exp_files)\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"exclude, exp\",\n",
    "    [\n",
    "        ([], exp_files),\n",
    "        ([\"I am not a Dir\"], exp_files),\n",
    "        ([\"dir1\"], exp_dir1),\n",
    "        ([\"dir2\"], exp_dir2),\n",
    "        ([\"subdir1\"], exp_subdir),\n",
    "        ([\"subdir1\", \"dir2\"], exp_dir2_subdir),\n",
    "    ],\n",
    ")\n",
    "def test_ls_exclude(exclude, exp, populated_tmp_dir):\n",
    "    \"\"\"Test `ls` function with an exclusion of some directories\"\"\"\n",
    "    df = ls(populated_tmp_dir, exclude=exclude)\n",
    "    assert sorted(df[\"Name\"]) == sorted(exp)\n",
    "\n",
    "\n",
    "def test_ls_no_info(populated_tmp_dir):\n",
    "    \"\"\"Test `ls` function with `hide_info` set to True\"\"\"\n",
    "    df = ls(populated_tmp_dir, hide_info=True)\n",
    "    assert set(exp_columns) - set(df.columns) == {\"Size\", \"Friendly_Size\"}\n",
    "\n",
    "def test_ls_not_exist():\n",
    "    \"\"\"Test `ls` when path does not exist\"\"\"\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        ls(\"this_path_does_not_exist\")\n",
    "\n",
    "def test_not_dir(tmpdir):\n",
    "    \"\"\"Test `ls` if root is a file and not a dir\"\"\"\n",
    "    file_path = Path(tmpdir) / \"toto.txt\"\n",
    "    file_path.touch(exist_ok=True)\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        ls(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphViz\n",
    "\n",
    "Drawing a Graphviz Graph in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gv(source: str, rankdir=\"LR\"):\n",
    "    \"\"\"Generate a GraphViz diagram, with a settable orientation (i.e. rankdir)\"\"\"\n",
    "    source = source.strip().rstrip(\";\")\n",
    "    return graphviz.Source(f'digraph G {{  rankdir=\"{rankdir}\" {source}; }}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# Let's try a bit\n",
    "gv(\"program[shape=box3d width=1 height=0.7] inputs->program->results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv(\"program[shape=box3d width=1 height=0.7] inputs->program->results\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "@pytest.mark.parametrize(\"rankdir\", [None, \"LR\", \"TD\"])\n",
    "def test_gv(rankdir):\n",
    "    \"\"\"Test Graphviz Generation via `gv`\"\"\"\n",
    "    src = \"inputs->program->results\"\n",
    "    if rankdir is None:\n",
    "        graph = gv(src)\n",
    "        rankdir = \"LR\"  # default orientation\n",
    "    else:\n",
    "        graph = gv(src, rankdir=rankdir)\n",
    "\n",
    "    assert isinstance(\n",
    "        graph, graphviz.Source\n",
    "    ), f\"Output shall be a GraphViz Source (but is {type(graph)})\"\n",
    "\n",
    "    gv_src = graph.source\n",
    "    assert gv_src.startswith(\"digraph\"), f\"Source is not a digraph: {gv_src}\"\n",
    "    assert src in gv_src, f\"Source input not in Graph Source: {gv_src}\"\n",
    "    assert f'rankdir=\"{rankdir}\"' in gv_src, f\"Rankdir {rankdir} not found: {gv_src}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & url_2_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_url_size(url: str) -> int:\n",
    "    \"\"\"Returns the size of URL, or -1 if it cannot get it\"\"\"\n",
    "    with requests.request(\"HEAD\", url) as resp:\n",
    "        if resp.status_code != 200:\n",
    "            raise ConnectionError(f\"Error when retrieving size from {url}\")\n",
    "\n",
    "        return int(resp.headers.get(\"Content-Length\", -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download(url: str, dest: PathStr = None) -> Path:\n",
    "    \"\"\"Download file and return the Path of the downloaded file\n",
    "\n",
    "    If the destination is not specified, download in the current directory\n",
    "    with the name specified in the URL\n",
    "    \"\"\"\n",
    "    # We can allow empty destination, in which case we use file name in URL\n",
    "    # We need to ensure that destination is a Path\n",
    "    dest = Path(dest or url.rpartition(\"/\")[-1])\n",
    "\n",
    "    # For big files, we will split into chunks\n",
    "    # We might also consider adding a progress bar\n",
    "    size = get_url_size(url)\n",
    "    if size < 1024 * 1024:\n",
    "        with requests.get(url) as resp:\n",
    "            resp.raise_for_status()\n",
    "            dest.write_bytes(resp.content)\n",
    "\n",
    "    else:\n",
    "        with requests.get(url, stream=True) as resp:\n",
    "            resp.raise_for_status()\n",
    "            with dest.open(\"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "    logger.info(f\"Downloaded {url} to {dest}\")\n",
    "    return dest\n",
    "\n",
    "\n",
    "def url_2_text(url: str) -> str:\n",
    "    \"\"\"Extract text content from an URL (textual content for an HTML)\"\"\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        raise ConnectionError(f\"Error when retrieving text content from {url}\")\n",
    "\n",
    "    resp.encoding = \"utf-8\"\n",
    "    content_type = resp.headers[\"Content-Type\"]\n",
    "    if \"html\" in content_type:\n",
    "        text = BeautifulSoup(resp.text).text\n",
    "    else:\n",
    "        text = resp.text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "GITHUB_TEST_DATA_URL = \"https://raw.githubusercontent.com/unpackAI/unpackai/main/test/test_data/\"\n",
    "url_raw_txt = f\"{GITHUB_TEST_DATA_URL}/to_download.txt\"\n",
    "test_data_txt = (DATA_DIR / \"to_download.txt\").read_text()\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def check_connection_github():\n",
    "    try:\n",
    "        with requests.request(\"HEAD\", url_raw_txt, timeout=1) as resp:\n",
    "            resp.raise_for_status()\n",
    "    except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout) as e:\n",
    "        pytest.xfail(f\"Cannot connect to Github: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "def test_get_url_size(check_connection_github):\n",
    "    assert get_url_size(url_raw_txt) == 264, f\"Wrong size for {url_raw_txt}\"\n",
    "\n",
    "\n",
    "def test_download_dest(check_connection_github, tmpdir):\n",
    "    \"\"\"Test download of file to a destination\"\"\"\n",
    "    dest = Path(tmpdir / \"to_download.txt\")\n",
    "    download(url_raw_txt, dest)\n",
    "    assert dest.is_file()\n",
    "    assert dest.read_text() == test_data_txt\n",
    "\n",
    "\n",
    "def test_download_empty(check_connection_github, tmpdir):\n",
    "    \"\"\"Test download of file without destination\"\"\"\n",
    "    dest = Path(\"to_download.txt\")\n",
    "    download(url_raw_txt)\n",
    "    try:\n",
    "        assert dest.is_file()\n",
    "        assert dest.read_text() == test_data_txt\n",
    "    finally:\n",
    "        dest.unlink()\n",
    "\n",
    "\n",
    "def test_url_2_text(check_connection_github):\n",
    "    \"\"\"Test extraction of text from URL\"\"\"\n",
    "    assert url_2_text(url_raw_txt) == test_data_txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack & Download-unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# We will re-use directly the unzip function from shutil\n",
    "# Since we want it in our \"all\", we just assign to itself so nbdev can detect it\n",
    "unpack_archive = shutil.unpack_archive\n",
    "\n",
    "# Equivalent to:\n",
    "# def unpack_archive(filename, extract_dir=None, format=None):\n",
    "#     return shutil.unpack_archive(filename, extract_dir=None, format=None)\n",
    "\n",
    "\n",
    "def download_and_unpack(url: str, extract_dir: PathStr = None, fmt: str = None) -> Path:\n",
    "    \"\"\"Download a file and unzip it. Returns the path of unzipped directory\n",
    "\n",
    "    Args:\n",
    "        url: URL of the archive to download\n",
    "        extract_dir: name of the target directory, where the archive is unpacked.\n",
    "                    If not provided, <working directory> / <archive name without extesion> is used.\n",
    "        fmt: archive format (one of \"zip\", \"tar\", \"gztar\", \"bztar\",or \"xztar\").\n",
    "                Or any other registered format. If not provided, it is guessed from extension.\n",
    "    \"\"\"\n",
    "    # We will store the downloaded archive in a temporary folder\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        dest = Path(tmpdirname) / url.split(\"?\")[0].rpartition(\"/\")[-1]\n",
    "        download(url, dest=dest)\n",
    "\n",
    "        if extract_dir is None:\n",
    "            extract_dir = Path.cwd() / dest.stem\n",
    "\n",
    "        unpack_archive(str(dest), extract_dir=str(extract_dir), format=fmt)\n",
    "        logger.info(f\"Extracted {url} to folder {dest}\")\n",
    "\n",
    "    return Path(extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "url_ar = (\n",
    "    r\"https://alaska.usgs.gov/data/landBirds/sewardPeninsula/2012/\"\n",
    "    \"avianHabitat_sewardPeninsula_McNew_2012.zip\"\n",
    ")\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"url\", [url_ar, url_ar + \"?x=123\"], ids=[\"url\", \"url?x=y\"])\n",
    "@pytest.mark.parametrize(\"dest\", [None, \"unzip_dir\"], ids=[\"no dest\", \"dest\"])\n",
    "def test_download_and_unpack(url, dest, tmpdir):\n",
    "    \"\"\"Test download and unzip with `download_and_unpack`\"\"\"\n",
    "    if dest is None:\n",
    "        dest = Path(url.split(\"?\")[0].rpartition(\"/\")[-1]).stem\n",
    "\n",
    "    extract_dir = Path(tmpdir / dest)\n",
    "    download_and_unpack(url, extract_dir=extract_dir)\n",
    "\n",
    "    df_files = ls(extract_dir)\n",
    "    obt_files = {k: v for k, v in zip(df_files.Name, df_files.Size)}\n",
    "\n",
    "    exp_files = {\n",
    "        \"avianHabitat_sewardPeninsula_McNew_2012.csv\": 60617,\n",
    "        \"avianHabitat_sewardPeninsula_McNew_2012.html\": 22883,\n",
    "        \"avianHabitat_sewardPeninsula_McNew_2012.xml\": 14408,\n",
    "    }\n",
    "    assert (\n",
    "        obt_files == exp_files\n",
    "    ), f\"Differences found in list of files:\\n{obt_files}\\nvs\\n{exp_files}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV in archive to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def _zip_csv_2_df(zip_path:Path, csv_path:PathStr) -> pd.DataFrame:\n",
    "    \"\"\"CSV in zip to DataFrame\"\"\"\n",
    "    with ZipFile(zip_path) as zf:\n",
    "        try:\n",
    "            with zf.open(str(csv_path)) as f_csv:\n",
    "                return pd.read_csv(f_csv)\n",
    "        except KeyError:\n",
    "            files = \"\\n\".join(f\" * {f}\" for f in zf.namelist() if f.lower().endswith(\".csv\"))\n",
    "            raise FileNotFoundError(\n",
    "                f'CSV file \"{csv_path}\" not found in \"{zip_path}\" '\n",
    "                f\"containing following CSV files:\\n{files}\"\n",
    "            ) from None\n",
    "\n",
    "\n",
    "def _tar_csv_2_df(tar_path:Path, csv_path:PathStr) -> pd.DataFrame:\n",
    "    \"\"\"CSV in tar to DataFrame\"\"\"\n",
    "    with tarfile.open(tar_path) as tf:\n",
    "        try:\n",
    "            csv_member = tf.getmember(str(csv_path))\n",
    "            return pd.read_csv(tf.extractfile(member=csv_member))\n",
    "        except KeyError:\n",
    "            files = \"\\n\".join(f\" * {f}\" for f in tf.getnames() if f.lower().endswith(\".csv\"))\n",
    "            raise FileNotFoundError(\n",
    "                f'CSV file \"{csv_path}\" not found in \"{tar_path}\" '\n",
    "                f\"containing following CSV files:\\n{files}\"\n",
    "            ) from None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_csv_from_zip(archive: PathURL, csv_path: PathStr) -> pd.DataFrame:\n",
    "    \"\"\"Load a CSV file inside a zip/tar and returns the equivalent pandas `DataFrame`\n",
    "\n",
    "    Args:\n",
    "        archive: path or URL of the zip (or tar/tag.gz) file\n",
    "        csv_path: path of csv relative to root of archive\n",
    "            Note: if root is a folder, csv_path shall include this path (e.g. \"archive/my_table.csv\")\n",
    "    \"\"\"\n",
    "    if Path(csv_path).suffix.lower() != \".csv\":\n",
    "        raise AttributeError(\n",
    "            f'2nd argument (\"csv_path\") shall be a CSV file but is \"{csv_path}\"'\n",
    "        )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        if isinstance(archive, str) and archive.startswith(\"http\"):\n",
    "            zip_path = Path(tmpdirname) / archive.split(\"?\")[0].rpartition(\"/\")[-1]\n",
    "            download(archive, dest=zip_path)\n",
    "        else:\n",
    "            zip_path = Path(archive)\n",
    "\n",
    "        extensions = \"\".join(zip_path.suffixes[-2:]).lower()\n",
    "        if extensions == \".zip\":\n",
    "            return _zip_csv_2_df(zip_path, csv_path)\n",
    "        elif extensions in (\".tar\", \".tar.gz\"):\n",
    "            return _tar_csv_2_df(zip_path, csv_path)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f'Archive shall be either .zip, .tar, or .tar.gz but is \"{zip_path}\"'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>company</th>\n",
       "      <th>some_integer</th>\n",
       "      <th>some_float</th>\n",
       "      <th>zone</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eldon Base for stackable storage shelf, platinum</td>\n",
       "      <td>Muhammed MacIntyre</td>\n",
       "      <td>3</td>\n",
       "      <td>38.94</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Storage &amp; Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.7 Cubic Foot Compact \"Cube\" Office Refrigera...</td>\n",
       "      <td>Barry French</td>\n",
       "      <td>293</td>\n",
       "      <td>208.16</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Appliances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Cardinal Slant-D? Ring Binder, Heavy Gauge Vinyl</td>\n",
       "      <td>Barry French</td>\n",
       "      <td>293</td>\n",
       "      <td>8.69</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Binders and Binder Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>R380</td>\n",
       "      <td>Clay Rozendal</td>\n",
       "      <td>483</td>\n",
       "      <td>195.99</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Telephones and Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Holmes HEPA Air Purifier</td>\n",
       "      <td>Carlos Soltero</td>\n",
       "      <td>515</td>\n",
       "      <td>21.78</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Appliances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Linden? 12\" Wall Clock With Oak Frame</td>\n",
       "      <td>Doug Bickford</td>\n",
       "      <td>10535</td>\n",
       "      <td>33.98</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Office Furnishings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Newell 326</td>\n",
       "      <td>Doug Bickford</td>\n",
       "      <td>10535</td>\n",
       "      <td>1.76</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Pens &amp; Art Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Prismacolor Color Pencil Set</td>\n",
       "      <td>Jamie Kunitz</td>\n",
       "      <td>10789</td>\n",
       "      <td>19.84</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Pens &amp; Art Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Xerox Blank Computer Paper</td>\n",
       "      <td>Anthony Johnson</td>\n",
       "      <td>10791</td>\n",
       "      <td>19.98</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>600 Series Flip</td>\n",
       "      <td>Ralph Knight</td>\n",
       "      <td>10945</td>\n",
       "      <td>95.99</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Telephones and Communication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               name  \\\n",
       "0     1   Eldon Base for stackable storage shelf, platinum   \n",
       "1     2  1.7 Cubic Foot Compact \"Cube\" Office Refrigera...   \n",
       "2     3   Cardinal Slant-D? Ring Binder, Heavy Gauge Vinyl   \n",
       "3     4                                               R380   \n",
       "4     5                           Holmes HEPA Air Purifier   \n",
       "..  ...                                                ...   \n",
       "95   96              Linden? 12\" Wall Clock With Oak Frame   \n",
       "96   97                                         Newell 326   \n",
       "97   98                       Prismacolor Color Pencil Set   \n",
       "98   99                         Xerox Blank Computer Paper   \n",
       "99  100                                    600 Series Flip   \n",
       "\n",
       "               company  some_integer  some_float                   zone  \\\n",
       "0   Muhammed MacIntyre             3       38.94                Nunavut   \n",
       "1         Barry French           293      208.16                Nunavut   \n",
       "2         Barry French           293        8.69                Nunavut   \n",
       "3        Clay Rozendal           483      195.99                Nunavut   \n",
       "4       Carlos Soltero           515       21.78                Nunavut   \n",
       "..                 ...           ...         ...                    ...   \n",
       "95       Doug Bickford         10535       33.98  Northwest Territories   \n",
       "96       Doug Bickford         10535        1.76  Northwest Territories   \n",
       "97        Jamie Kunitz         10789       19.84  Northwest Territories   \n",
       "98     Anthony Johnson         10791       19.98  Northwest Territories   \n",
       "99        Ralph Knight         10945       95.99  Northwest Territories   \n",
       "\n",
       "                              type  \n",
       "0           Storage & Organization  \n",
       "1                       Appliances  \n",
       "2   Binders and Binder Accessories  \n",
       "3     Telephones and Communication  \n",
       "4                       Appliances  \n",
       "..                             ...  \n",
       "95              Office Furnishings  \n",
       "96             Pens & Art Supplies  \n",
       "97             Pens & Art Supplies  \n",
       "98                           Paper  \n",
       "99    Telephones and Communication  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "read_csv_from_zip(DATA_DIR / \"archive.zip\", \"100_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>company</th>\n",
       "      <th>some_integer</th>\n",
       "      <th>some_float</th>\n",
       "      <th>zone</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eldon Base for stackable storage shelf, platinum</td>\n",
       "      <td>Muhammed MacIntyre</td>\n",
       "      <td>3</td>\n",
       "      <td>38.94</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Storage &amp; Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.7 Cubic Foot Compact \"Cube\" Office Refrigera...</td>\n",
       "      <td>Barry French</td>\n",
       "      <td>293</td>\n",
       "      <td>208.16</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Appliances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Cardinal Slant-D? Ring Binder, Heavy Gauge Vinyl</td>\n",
       "      <td>Barry French</td>\n",
       "      <td>293</td>\n",
       "      <td>8.69</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Binders and Binder Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>R380</td>\n",
       "      <td>Clay Rozendal</td>\n",
       "      <td>483</td>\n",
       "      <td>195.99</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Telephones and Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Holmes HEPA Air Purifier</td>\n",
       "      <td>Carlos Soltero</td>\n",
       "      <td>515</td>\n",
       "      <td>21.78</td>\n",
       "      <td>Nunavut</td>\n",
       "      <td>Appliances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Linden? 12\" Wall Clock With Oak Frame</td>\n",
       "      <td>Doug Bickford</td>\n",
       "      <td>10535</td>\n",
       "      <td>33.98</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Office Furnishings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Newell 326</td>\n",
       "      <td>Doug Bickford</td>\n",
       "      <td>10535</td>\n",
       "      <td>1.76</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Pens &amp; Art Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Prismacolor Color Pencil Set</td>\n",
       "      <td>Jamie Kunitz</td>\n",
       "      <td>10789</td>\n",
       "      <td>19.84</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Pens &amp; Art Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Xerox Blank Computer Paper</td>\n",
       "      <td>Anthony Johnson</td>\n",
       "      <td>10791</td>\n",
       "      <td>19.98</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>600 Series Flip</td>\n",
       "      <td>Ralph Knight</td>\n",
       "      <td>10945</td>\n",
       "      <td>95.99</td>\n",
       "      <td>Northwest Territories</td>\n",
       "      <td>Telephones and Communication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               name  \\\n",
       "0     1   Eldon Base for stackable storage shelf, platinum   \n",
       "1     2  1.7 Cubic Foot Compact \"Cube\" Office Refrigera...   \n",
       "2     3   Cardinal Slant-D? Ring Binder, Heavy Gauge Vinyl   \n",
       "3     4                                               R380   \n",
       "4     5                           Holmes HEPA Air Purifier   \n",
       "..  ...                                                ...   \n",
       "95   96              Linden? 12\" Wall Clock With Oak Frame   \n",
       "96   97                                         Newell 326   \n",
       "97   98                       Prismacolor Color Pencil Set   \n",
       "98   99                         Xerox Blank Computer Paper   \n",
       "99  100                                    600 Series Flip   \n",
       "\n",
       "               company  some_integer  some_float                   zone  \\\n",
       "0   Muhammed MacIntyre             3       38.94                Nunavut   \n",
       "1         Barry French           293      208.16                Nunavut   \n",
       "2         Barry French           293        8.69                Nunavut   \n",
       "3        Clay Rozendal           483      195.99                Nunavut   \n",
       "4       Carlos Soltero           515       21.78                Nunavut   \n",
       "..                 ...           ...         ...                    ...   \n",
       "95       Doug Bickford         10535       33.98  Northwest Territories   \n",
       "96       Doug Bickford         10535        1.76  Northwest Territories   \n",
       "97        Jamie Kunitz         10789       19.84  Northwest Territories   \n",
       "98     Anthony Johnson         10791       19.98  Northwest Territories   \n",
       "99        Ralph Knight         10945       95.99  Northwest Territories   \n",
       "\n",
       "                              type  \n",
       "0           Storage & Organization  \n",
       "1                       Appliances  \n",
       "2   Binders and Binder Accessories  \n",
       "3     Telephones and Communication  \n",
       "4                       Appliances  \n",
       "..                             ...  \n",
       "95              Office Furnishings  \n",
       "96             Pens & Art Supplies  \n",
       "97             Pens & Art Supplies  \n",
       "98                           Paper  \n",
       "99    Telephones and Communication  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "read_csv_from_zip(DATA_DIR / \"archive.tar.gz\", \"100_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportest\n",
    "LOCAL_ZIP_FLAT = DATA_DIR / \"archive.zip\"\n",
    "LOCAL_TAR_FLAT = DATA_DIR / \"archive.tar\"\n",
    "LOCAL_GZ_FLAT = DATA_DIR / \"archive.tar.gz\"\n",
    "LOCAL_ZIP_FOLDER = DATA_DIR / \"archived_folder.zip\"\n",
    "GITHUB_ZIP_FLAT = f\"{GITHUB_TEST_DATA_URL}/archive.zip\"\n",
    "GITHUB_ZIP_FOLDER = f\"{GITHUB_TEST_DATA_URL}/archived_folder.zip\"\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"archive,csv\",\n",
    "    [\n",
    "        (LOCAL_ZIP_FLAT, \"100_rows.csv\"),\n",
    "        (LOCAL_TAR_FLAT, \"100_rows.csv\"),\n",
    "        (LOCAL_GZ_FLAT, \"100_rows.csv\"),\n",
    "        (LOCAL_ZIP_FOLDER, \"archived_folder/100_rows (folder).csv\"),\n",
    "        (LOCAL_ZIP_FOLDER, \"archived_folder/sub_folder/100_rows (subfolder).csv\"),\n",
    "    ],\n",
    "    ids=[\"flat (zip)\", \"flat (tar)\", \"flat (tar.gz)\", \"folder\", \"subfolder\"],\n",
    ")\n",
    "def test_read_csv_from_zip_local(archive, csv):\n",
    "    \"\"\"Test reading CSV from a local zip with read_csv_from_zip\"\"\"\n",
    "    df = read_csv_from_zip(archive, csv)\n",
    "    assert isinstance(df, pd.DataFrame), f\"Result is not a DataFrame: {df}\"\n",
    "    assert len(df) == 100\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"archive,csv\",\n",
    "    [\n",
    "        (GITHUB_ZIP_FLAT, \"100_rows.csv\"),\n",
    "        (GITHUB_ZIP_FOLDER, \"archived_folder/100_rows (folder).csv\"),\n",
    "        (GITHUB_ZIP_FOLDER, \"archived_folder/sub_folder/100_rows (subfolder).csv\"),\n",
    "    ],\n",
    "    ids=[\"flat\", \"folder\", \"subfolder\"],\n",
    ")\n",
    "def test_read_csv_from_zip_github(archive, csv, check_connection_github):\n",
    "    \"\"\"Test reading CSV from a URL zip in GitHub with read_csv_from_zip\"\"\"\n",
    "    df = read_csv_from_zip(archive, csv)\n",
    "    assert isinstance(df, pd.DataFrame), f\"Result is not a DataFrame: {df}\"\n",
    "    assert len(df) == 100\n",
    "\n",
    "\n",
    "def test_read_csv_from_zip_url():\n",
    "    \"\"\"Test reading CSV from a URL zip with read_csv_from_zip\"\"\"\n",
    "    url_ar = (\n",
    "        r\"https://alaska.usgs.gov/data/landBirds/sewardPeninsula/2012/\"\n",
    "        \"avianHabitat_sewardPeninsula_McNew_2012.zip\"\n",
    "    )\n",
    "    df = read_csv_from_zip(url_ar, \"avianHabitat_sewardPeninsula_McNew_2012.csv\")\n",
    "    assert isinstance(df, pd.DataFrame), f\"Result is not a DataFrame: {df}\"\n",
    "    assert len(df) == 1070\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"archive,csv,error\",\n",
    "    [\n",
    "        (\"does_not_exist.zip\", \"table.csv\", FileNotFoundError),\n",
    "        (LOCAL_ZIP_FLAT, \"does_not_exist.csv\", FileNotFoundError),\n",
    "        (LOCAL_TAR_FLAT, \"does_not_exist.csv\", FileNotFoundError),\n",
    "        (LOCAL_GZ_FLAT, \"does_not_exist.csv\", FileNotFoundError),\n",
    "        (LOCAL_ZIP_FLAT, \"not_a_csv.txt\", AttributeError),\n",
    "        (LOCAL_ZIP_FLAT, \"not_a_csv\", AttributeError),\n",
    "        (DATA_DIR / \"to_download.txt\", \"table.csv\", AttributeError),\n",
    "    ],\n",
    "    ids=[\n",
    "        \"zip missing\",\n",
    "        \"csv missing (zip)\",\n",
    "        \"csv missing (tar)\",\n",
    "        \"csv missing (tar.gz)\",\n",
    "        \"not csv (extension)\",\n",
    "        \"not csv (no extension)\",\n",
    "        \"not archive\",\n",
    "    ],\n",
    ")\n",
    "def test_read_csv_from_zip_robustness(archive, csv, error):\n",
    "    \"\"\"Test robustness of read_csv_from_zip\"\"\"\n",
    "    with pytest.raises(error):\n",
    "        read_csv_from_zip(archive, csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Hide -->\n",
    "# Running all Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "try:\n",
    "    ishell = get_ipython()\n",
    "except NameError as e:\n",
    "    from IPython.testing.globalipapp import get_ipython, start_ipython\n",
    "    ishell = start_ipython()\n",
    "    print(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                            [100%]\u001b[0m##vso[results.publish type=JUnit;runTitle='Pytest results';]e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml\n",
      "##vso[task.logissue type=warning;]Coverage XML was not created, skipping upload.\n",
      "\n",
      "------------ generated xml file: e:\\AnsysDev\\_perso_repo\\unpackai\\nbs\\test-output.xml -------------\n",
      "\u001b[32m\u001b[32m\u001b[1m17 passed\u001b[0m\u001b[32m in 0.35s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
